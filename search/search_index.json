{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PRISTINE: Profile-based Inference for Statistical Networks","text":"<p>PRISTINE is a modular framework for phylogenetic likelihood inference, built in PyTorch \u2265 2.6 and Python \u2265 3.10.</p> <p>It supports:</p> <ul> <li>Efficient likelihood computation with Felsenstein's pruning algorithm</li> <li>Model-based simulation and inference (e.g., GTR, BDS)</li> <li>Parameter profiling and Laplace approximation</li> <li>Relaxed and strict molecular clocks</li> <li>Transparent and flexible parameter management</li> </ul>"},{"location":"#quick-start","title":"\ud83d\udccc Quick Start","text":"<ol> <li>Install locally:</li> </ol> <pre><code>pip install -e .\n</code></pre> <ol> <li> <p>Run a minimal inference: <pre><code>from pristine import optimize, binarytree, gtr\n...\n</code></pre></p> </li> <li> <p>See tutorials and covered scientific questions</p> </li> </ol>"},{"location":"#components","title":"\ud83d\udce6 Components","text":"Module Description <code>felsenstein.py</code> FPA-based log-likelihood engine <code>gtr.py</code> GTR substitution model <code>sequence.py</code> Sequence simulation and pattern collapse <code>molecularclock.py</code> Strict and relaxed clock models <code>laplace_estimator.py</code> Laplace approximation of uncertainty <code>likelihood_profiler.py</code> Confidence intervals via profiling <code>binarytree.py</code> Tree structure and simulation <code>edgelist.py</code> Time-aware representation of trees"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#development-mode","title":"Development mode","text":"<pre><code>git clone https://github.com/rasigadelab/pristine.git\ncd pristine\npip install -e .\n</code></pre>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python \u2265 3.10</li> <li>PyTorch \u2265 2.6</li> <li>NumPy, pandas, matplotlib</li> </ul>"},{"location":"math/bds_model/","title":"Birth-Death-Sampling (BDS) Models","text":""},{"location":"math/bds_model/#overview","title":"Overview","text":"<p>The <code>bds_model.py</code> module implements phylogenetic models based on birth-death-sampling (BDS) processes. These models describe how lineages speciate (birth), go extinct (death), and are sampled over evolutionary time. They are particularly suited for modeling time-calibrated trees under epidemiological or macroevolutionary scenarios.</p> <p>The module includes: - Time-homogeneous and state-dependent BDS likelihoods - Analytical expressions for survival probabilities (<code>q(t)</code>) and extinction probabilities (<code>p\u2080(t)</code>) - Simulation utilities for stochastic tree generation - Extensions with linear state-dependent birth rates</p>"},{"location":"math/bds_model/#core-quantities","title":"Core Quantities","text":"<p>Let \\(b\\), \\(d\\), and \\(s\\) be the birth, death, and sampling rates. The Stadler q-function:</p> \\[ q(t) = 2(1 - c^2) + e^{-ct}(1 - c)^2 + e^{ct}(1 + c)^2 \\] <p>with:</p> \\[ c = \\frac{-(b - d - s)}{\\sqrt{(b - d - s)^2 + 4bs}} \\] <p>governs the contribution of each branch to the tree likelihood\u3010123\u2020source\u3011.</p> <p>The extinction probability \\(p_0(t)\\) is:</p> \\[ p_0(t) = \\frac{b + d + s + c \\cdot \\frac{e^{-ct}(1 - c) - (1 + c)}{e^{-ct}(1 - c) + (1 + c)}}{2b} \\]"},{"location":"math/bds_model/#tree-likelihood-homogeneous-model","title":"Tree Likelihood: Homogeneous Model","text":"<p>For a tree with \\(E\\) edges and \\(T\\) tips, the log-likelihood is:</p> \\[ \\log \\mathcal{L} = \\sum_{e \\in \\text{edges}} \\log \\frac{q(t_e^\\text{child})}{q(t_e^\\text{parent})} + (T-1)\\log b + T \\log s \\] <p>Implemented by:</p> <pre><code>BirthDeathSamplingTreeWise.log_likelihood(treecal)\n</code></pre>"},{"location":"math/bds_model/#state-dependent-bds","title":"State-Dependent BDS","text":"<p>Allows birth, death, and sampling to vary by hidden discrete state. Given per-edge parent state probabilities \\(p_k\\), the likelihood term is:</p> \\[ \\sum_{e \\in \\text{edges}} \\log \\left( \\sum_k p_k \\cdot \\frac{q_k(t_\\text{child})}{q_k(t_\\text{parent})} \\cdot r_k \\right) \\] <p>where \\(r_k\\) is \\(b_k\\) or \\(s_k\\) depending on whether the child is internal or a tip.</p> <p>Implemented by:</p> <pre><code>StateDependentBirthDeathSampling.log_likelihood(treecal, ancestor_states)\n</code></pre>"},{"location":"math/bds_model/#simulation-birthdeathsamplingsimulator","title":"Simulation: BirthDeathSamplingSimulator","text":"<p>Stochastically grows a tree forward in time using specified BDS rates. Events: - Birth: splits lineage into two children - Death: terminates lineage - Sampling: marks lineage as a tip</p> <p>Simulation continues until a target number of tips is sampled\u3010123\u2020source\u3011.</p>"},{"location":"math/bds_model/#extensions-linear-marker-birth-model","title":"Extensions: Linear Marker Birth Model","text":"<p>The <code>LinearMarkerBirthModel</code> allows the birth rate to depend linearly on hidden marker states:</p> \\[ b_n = \\exp\\left(\\beta_0 + \\sum_{m,k&gt;0} \\beta_{mk} \\cdot p_{n,m,k}\\right) \\] <p>This enables modeling trait-dependent diversification. The log-likelihood aggregates over edges:</p> \\[ \\sum_e \\log \\left( \\frac{q_{\\text{child}}}{q_{\\text{parent}}} \\right) + \\sum_{\\text{birth events}} \\log b + \\sum_{\\text{tips}} \\log s \\]"},{"location":"math/bds_model/#api-summary","title":"API Summary","text":""},{"location":"math/bds_model/#likelihood-classes","title":"Likelihood Classes","text":"<ul> <li><code>BirthDeathSamplingTreeWise</code>: homogeneous log-likelihood</li> <li><code>StateDependentBirthDeathSampling</code>: per-state likelihood with posterior weighting</li> <li><code>LinearMarkerBirthModel</code>: linear trait-dependent birth rate</li> </ul>"},{"location":"math/bds_model/#simulation","title":"Simulation","text":"<ul> <li><code>BirthDeathSamplingSimulator</code>: forward-in-time stochastic simulation</li> <li><code>BirthDeathSamplingNodeData</code>: holds per-node rates</li> </ul>"},{"location":"math/bds_model/#utilities","title":"Utilities","text":"<ul> <li><code>stadler_q(t, b, d, s)</code>: scalar q function</li> <li><code>stadler_q_matrix</code>: vectorized q(t) over nodes and states</li> <li><code>stadler_p0</code>: extinction probability</li> </ul>"},{"location":"math/bds_model/#references","title":"References","text":"<ul> <li>Stadler, T. (2010). Sampling-through-time in birth\u2013death trees. JTB.</li> <li>Maddison, W. P., et al. (2007). Estimating a binary character\u2019s effect on speciation.</li> <li>Rabosky, D. L. (2014). Automatic detection of key innovations.</li> </ul>"},{"location":"math/bds_model/#source","title":"Source","text":"<p>Defined in <code>bds_model.py</code>\u3010123\u2020source\u3011.</p>"},{"location":"math/felsenstein/","title":"Felsenstein Pruning Algorithm (FPA)","text":""},{"location":"math/felsenstein/#overview","title":"Overview","text":"<p>The Felsenstein pruning algorithm (FPA) is a dynamic programming method to compute the likelihood of observed sequences at the tips of a phylogenetic tree under a substitution model. It works by traversing the tree from the tips to the root in post-order, propagating conditional likelihoods and applying transition probabilities computed from a rate matrix.</p> <p>In the PRISTINE framework, the <code>FelsensteinPruningAlgorithm</code> class implements this procedure in a differentiable, TorchScript-compatible form. It supports both probability-space and log-space formulations, and handles multiple unique site patterns efficiently.</p>"},{"location":"math/felsenstein/#mathematical-foundations","title":"Mathematical Foundations","text":"<p>Let: - \\(L\\) be the number of alignment sites - \\(K\\) be the number of possible states (e.g. 4 for DNA) - \\(N\\) be the number of nodes in the tree - \\(P(t) = e^{Qt}\\) be the transition matrix over duration \\(t\\) with rate matrix \\(Q\\)</p> <p>Let \\(f_{n\\ell k}\\) be the conditional likelihood that node \\(n\\) emits state \\(k\\) at site \\(\\ell\\). For leaves, this is a one-hot vector based on observed data. For internal nodes, we compute:</p> \\[ f_{n\\ell k} = \\prod_{c \\in  ext{children}(n)} \\sum_{j=1}^K P_{kj}(t_{nc}) f_{c\\ell j} \\] <p>At the root, the marginal likelihood per site is:</p> \\[ \\mathcal{L}_\\ell = \\sum_{k=1}^K \\pi_k f_{0\\ell k} \\] <p>The overall log-likelihood is then:</p> \\[ \\log \\mathcal{L} = \\sum_{\\ell=1}^L \\log \\mathcal{L}_\\ell \\]"},{"location":"math/felsenstein/#algorithm-details","title":"Algorithm Details","text":"<p>The FPA implementation in PRISTINE uses the following approach:</p>"},{"location":"math/felsenstein/#1-tree-recursion-structure","title":"1. Tree Recursion Structure","text":"<p>Edges are grouped into recursion levels using <code>get_postorder_edge_list</code>, ensuring that edges in each level do not depend on those in later levels\u301065\u2020source\u3011.</p>"},{"location":"math/felsenstein/#2-forward-evaluation-probability-space","title":"2. Forward Evaluation (Probability Space)","text":"<ul> <li>Initialization: Tip nodes use observed sequences; internal nodes are initialized uniformly.</li> <li>Edge messages: For each edge, compute:</li> </ul> \\[     ext{msg}_{e,\\ell,k} = \\sum_j f_{c\\ell j} P_{jk}(t_e) \\] <ul> <li>Accumulation: Log-messages from children are accumulated at parents:</li> </ul> \\[ \\log f_{p\\ell k} = \\log f_{p\\ell k} + \\sum_{c} \\log     ext{msg}_{c     o p} \\] <ul> <li> <p>Normalization: After accumulation, the conditional likelihood at each node is normalized and the scale factor is stored in <code>log_scaling</code>.</p> </li> <li> <p>Final likelihood: At the root, compute the total log-likelihood across all sites with pattern weights\u301065\u2020source\u3011.</p> </li> </ul>"},{"location":"math/felsenstein/#3-log-space-variant","title":"3. Log-Space Variant","text":"<p>To improve numerical stability, the method <code>log_likelihood_logspace_</code> implements the full algorithm in log-space:</p> <ul> <li>Likelihoods are propagated as \\(\\log f\\)</li> <li>Multiplications become additions</li> <li>Sums become log-sum-exp operations</li> </ul> <p>This is more robust for long trees or many sites, but slightly slower\u301065\u2020source\u3011.</p>"},{"location":"math/felsenstein/#numerical-stability","title":"Numerical Stability","text":"<p>To avoid underflow, both the probability-space and log-space implementations use per-site log-scaling terms. These scalings are stored and accumulated during recursion, ensuring that likelihood values remain in a computable range:</p> \\[     ext{CL}_{   ext{true}} = \\exp(  ext{log\\_scaling}) \\cdot    ext{node\\_probs} \\]"},{"location":"math/felsenstein/#output-and-posterior-states","title":"Output and Posterior States","text":"<p>The field <code>ancestor_states</code> stores the posterior state probabilities at internal nodes. These are available for downstream tasks such as:</p> <ul> <li>Ancestral reconstruction</li> <li>Trait correlation with divergence</li> <li>Structured birth-death likelihoods</li> </ul>"},{"location":"math/felsenstein/#practical-considerations","title":"Practical Considerations","text":"<ul> <li>Vectorization: All site computations are vectorized over alignment patterns.</li> <li>Batch support: Transition matrices are precomputed in batches for each edge.</li> <li>Tree traversal: Performed in level-by-level recursion groups for parallelization.</li> </ul>"},{"location":"math/felsenstein/#references","title":"References","text":"<ul> <li>Felsenstein, J. (1981). Evolutionary trees from DNA sequences: a maximum likelihood approach. Journal of Molecular Evolution.</li> <li>Yang, Z. (2014). Molecular Evolution: A Statistical Approach. Oxford University Press.</li> </ul>"},{"location":"math/felsenstein/#source","title":"Source","text":"<p>Defined in <code>felsenstein.py</code>\u301065\u2020source\u3011.</p>"},{"location":"math/gtr_model/","title":"Generalized Time-Reversible (GTR) Model","text":""},{"location":"math/gtr_model/#overview","title":"Overview","text":"<p>The GTR model is a flexible and widely used continuous-time Markov model (CTMM) for modeling sequence evolution in phylogenetics. It defines a stationary and time-reversible substitution process over a finite set of states (e.g., nucleotides or amino acids).</p> <p>The <code>GeneralizedTimeReversibleModel</code> class in the PRISTINE framework provides a fully differentiable implementation of this model. It supports gradient-based estimation of evolutionary parameters directly from data and provides efficient routines for computing matrix exponentials needed in likelihood calculations.</p> <p>This model is parameterized by: - A stationary distribution \\(\\boldsymbol{\\pi}\\) over the \\(K\\) states - A symmetric matrix of exchange rates \\(R\\) between state pairs \\((i, j)\\)</p> <p>The rate matrix \\(Q\\) is then derived from these components.</p>"},{"location":"math/gtr_model/#continuous-time-markov-models-ctmm","title":"Continuous-Time Markov Models (CTMM)","text":"<p>In a CTMM, the probability of transitioning from state \\(i\\) to \\(j\\) over time \\(t\\) is described by the matrix exponential of a rate matrix \\(Q\\):</p> \\[ P(t) = \\exp(Qt) \\] <p>The entries of \\(Q\\) satisfy: - \\(Q_{ij} \\geq 0\\) for \\(i \\neq j\\) (substitution rates) - \\(\\sum_{j} Q_{ij} = 0\\) for all \\(i\\) (rows sum to zero)</p> <p>The diagonal entries are defined as:</p> \\[ Q_{ii} = -\\sum_{j \\neq i} Q_{ij} \\]"},{"location":"math/gtr_model/#gtr-parameterization","title":"GTR Parameterization","text":"<p>The GTR model assumes time-reversibility, i.e., \\(\\pi_i Q_{ij} = \\pi_j Q_{ji}\\).</p> <p>It is parameterized via: - \\(\\boldsymbol{\\pi}\\): stationary distribution (a probability vector) - \\(R\\): symmetric exchangeability matrix</p> <p>For \\(i \\neq j\\), the off-diagonal rate is:</p> \\[ Q_{ij} = \\pi_j R_{ij} \\] <p>Diagonal entries are set to ensure rows sum to zero:</p> \\[ Q_{ii} = -\\sum_{j \\neq i} Q_{ij} \\] <p>This guarantees that \\(\\boldsymbol{\\pi}\\) is the stationary distribution of \\(Q\\).</p>"},{"location":"math/gtr_model/#stationary-distribution","title":"Stationary Distribution","text":"<p>Instead of directly optimizing \\(\\pi\\), the GTR model uses an unconstrained parameterization via logits \\(\\boldsymbol{\\ell} \\in \\mathbb{R}^{K-1}\\). The first component of \\(\\pi\\) is pinned to act as reference:</p> \\[ \\pi_0 = \\frac{1}{1 + \\sum_{i=1}^{K-1} \\exp(\\ell_i)}, \\quad \\pi_i = \\frac{\\exp(\\ell_i)}{1 + \\sum_{j=1}^{K-1} \\exp(\\ell_j)}, \\quad i = 1,\\dots,K-1 \\] <p>This ensures \\(\\pi\\) is a valid probability distribution (non-negative and sums to one) while reducing redundancy.</p>"},{"location":"math/gtr_model/#exchangeability-matrix-r","title":"Exchangeability Matrix \\(R\\)","text":"<p>Only the upper triangle (excluding diagonal) of \\(R\\) is parameterized. These values are exponentiated for positivity and symmetrically assigned:</p> \\[ R_{ij} = R_{ji} = \\exp(r_{ij}), \\quad i &lt; j \\] <p>Then the full \\(Q\\) matrix is constructed as described above.</p>"},{"location":"math/gtr_model/#implementation-details","title":"Implementation Details","text":"<p>The <code>GeneralizedTimeReversibleModel</code> class supports:</p> <ul> <li><code>stationary_dist()</code>: returns \\(\\boldsymbol{\\pi}\\)</li> <li><code>rate_matrix_stationary_dist()</code>: returns the full rate matrix \\(Q\\) and \\(\\boldsymbol{\\pi}\\)</li> <li><code>average_rate()</code>: computes the mean substitution rate \\(\\mu = -\\sum_i \\pi_i Q_{ii}\\)</li> <li><code>compute_batch_matrix_exp(durations)</code>: efficiently computes \\(\\exp(Qt)\\) for a batch of durations</li> </ul> <p>Internally, the model builds \\(Q\\) via:</p> <pre><code>Q = pi.unsqueeze(0) * R\nQ.fill_diagonal_(0.0)\nQ.diagonal().copy_(-Q.sum(dim=1))\n</code></pre> <p>which guarantees that \\(Q\\) is valid and satisfies detailed balance.</p>"},{"location":"math/gtr_model/#practical-usage","title":"Practical Usage","text":"<p>This model is typically used with the Felsenstein pruning algorithm to compute likelihoods on phylogenetic trees. In the PRISTINE framework, it integrates with:</p> <ul> <li><code>FelsensteinPruningAlgorithm</code> for sequence likelihoods</li> <li><code>SequenceSimulationVisitor</code> for simulating sequence evolution</li> <li>Molecular clock models for dating trees</li> </ul> <p>The GTR model is essential for inferring both phylogenies and substitution dynamics from sequence data.</p>"},{"location":"math/gtr_model/#references","title":"References","text":"<ul> <li>Yang, Z. (2014). Molecular Evolution: A Statistical Approach. Oxford University Press.</li> <li>Felsenstein, J. (1981). Evolutionary trees from DNA sequences: a maximum likelihood approach. Journal of Molecular Evolution.</li> </ul>"},{"location":"math/gtr_model/#source","title":"Source","text":"<p>Defined in <code>gtr.py</code>\u301026\u2020source\u3011.</p>"},{"location":"math/laplace/","title":"Laplace Approximation for Parameter Uncertainty","text":""},{"location":"math/laplace/#overview","title":"Overview","text":"<p>The <code>LaplaceEstimator</code> class implements a second-order approximation method for quantifying uncertainty in model parameters. Around a local optimum of the loss function (typically the negative log-likelihood), the function is approximated as quadratic, implying a multivariate Gaussian posterior distribution over parameters.</p> <p>The Laplace approximation enables the estimation of: - Marginal parameter variances - Confidence intervals - Local curvature (eigenvalue spectrum)</p> <p>It supports both exact Hessian inversion for small models and a scalable approximation (Hutchinson\u2019s method) for larger models.</p>"},{"location":"math/laplace/#mathematical-background","title":"Mathematical Background","text":"<p>Let \\(\\mathcal{L}(\\boldsymbol{\\theta})\\) be the negative log-likelihood. Near the maximum likelihood estimate (MLE) \\(\\hat{\\boldsymbol{\\theta}}\\), we approximate:</p> \\[ \\mathcal{L}(\\boldsymbol{\\theta}) \\approx \\mathcal{L}(\\hat{\\boldsymbol{\\theta}}) + \\frac{1}{2} (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}})^T H (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}}) \\] <p>where \\(H\\) is the Hessian of the loss at \\(\\hat{\\boldsymbol{\\theta}}\\). The posterior is then:</p> \\[ p(\\boldsymbol{\\theta} \\mid \\text{data}) \\approx \\mathcal{N}(\\hat{\\boldsymbol{\\theta}}, H^{-1}) \\]"},{"location":"math/laplace/#marginal-variance","title":"Marginal Variance","text":"<p>The marginal variance of parameter \\(\\theta_i\\) is:</p> \\[ \\text{Var}[\\theta_i] \\approx \\left[H^{-1}\\right]_{ii} \\] <p>Confidence intervals are estimated as:</p> \\[ \\theta_i \\pm z_{\\alpha/2} \\cdot \\sqrt{\\left[H^{-1}\\right]_{ii}} \\] <p>for a given confidence level (e.g., \\(z_{0.975} = 1.96\\) for 95%).</p>"},{"location":"math/laplace/#estimation-methods","title":"Estimation Methods","text":""},{"location":"math/laplace/#exact-dense-hessian","title":"Exact Dense Hessian","text":"<p>For small models, the full Hessian \\(H\\) is computed and inverted:</p> <pre><code>H = compute_dense_hessian()\nHinv = torch.linalg.inv(H)\n</code></pre> <p>The diagonal of \\(H^{-1}\\) gives marginal variances\u301067\u2020source\u3011.</p>"},{"location":"math/laplace/#hutchinsons-estimator-large-models","title":"Hutchinson\u2019s Estimator (Large Models)","text":"<p>When \\(H\\) is too large to invert, the estimator uses Hutchinson\u2019s method:</p> <ol> <li>Sample random vectors \\(v_i \\sim \\text{Rademacher}\\)</li> <li>Solve \\(H x_i = v_i\\) using conjugate gradient</li> <li>Estimate:</li> </ol> \\[ \\text{diag}(H^{-1}) \\approx rac{1}{M} \\sum_{i=1}^M v_i \\odot x_i \\] <p>This avoids computing or storing the full Hessian\u301067\u2020source\u3011.</p>"},{"location":"math/laplace/#api-highlights","title":"API Highlights","text":"<ul> <li><code>estimate_inv_hessian_diag()</code>: estimates the full diagonal of \\(H^{-1}\\)</li> <li><code>estim_variance_by_name(name)</code>: returns variance for a named parameter</li> <li><code>estim_all_confint_dict()</code>: returns 95% confidence intervals</li> <li><code>curvature_report()</code>: eigenvalue analysis of \\(H\\) to detect sloppy or non-identifiable directions\u301067\u2020source\u3011</li> </ul>"},{"location":"math/laplace/#curvature-analysis","title":"Curvature Analysis","text":"<p>To detect identifiability issues, eigenvalue bounds of \\(H\\) are estimated:</p> <ul> <li>Largest eigenvalue: via power iteration</li> <li>Smallest eigenvalue: via inverse iteration</li> </ul> <p>The flatness ratio is:</p> \\[ \\frac{\\lambda_{\\min}}{\\lambda_{\\max}} \\] <p>Interpretation: - \\(\\gtrsim 10^{-2}\\): well-conditioned - \\(10^{-6} \\lesssim \\cdot \\lesssim 10^{-2}\\): mild sloppiness - \\(\\ll 10^{-6}\\): severe non-identifiability\u301067\u2020source\u3011</p>"},{"location":"math/laplace/#limitations","title":"Limitations","text":"<ul> <li>Assumes the loss is locally quadratic</li> <li>Fails under flat or ill-posed likelihoods</li> <li>Hutchinson estimates are approximate and depend on sample size</li> </ul>"},{"location":"math/laplace/#references","title":"References","text":"<ul> <li>MacKay, D. J. C. (2003). Information Theory, Inference, and Learning Algorithms. Cambridge University Press.</li> <li>Rasmussen &amp; Williams (2006). Gaussian Processes for Machine Learning.</li> <li>Bishop, C. M. (2006). Pattern Recognition and Machine Learning.</li> </ul>"},{"location":"math/laplace/#source","title":"Source","text":"<p>Defined in <code>laplace_estimator.py</code>\u301067\u2020source\u3011.</p>"},{"location":"math/likelihood_profiler/","title":"Likelihood Profiling","text":"<p>\\</p>"},{"location":"math/likelihood_profiler/#likelihood-profiler","title":"Likelihood Profiler","text":""},{"location":"math/likelihood_profiler/#overview","title":"Overview","text":"<p>The <code>LikelihoodProfiler</code> class estimates confidence intervals for model parameters using likelihood profiling. Unlike local curvature-based methods (e.g. Laplace approximation), it evaluates how the likelihood changes when a single parameter is perturbed, while re-optimizing all other parameters.</p> <p>This approach is more robust for non-quadratic or asymmetric likelihood surfaces and is especially suited for maximum likelihood inference where parameters are interdependent or poorly constrained.</p>"},{"location":"math/likelihood_profiler/#likelihood-profiling","title":"Likelihood Profiling","text":"<p>Let \\(\\hat{\\theta}\\) be the maximum likelihood estimate of a parameter, and let \\(\\mathcal{L}(\\theta)\\) be the log-likelihood at that value. The confidence interval at level \\(1 - \\alpha\\) is estimated by solving:</p> \\[ \\mathcal{L}(\\theta) \\geq \\mathcal{L}(\\hat{\\theta}) - \\frac{1}{2} \\chi^2_{1, 1-\\alpha} \\] <p>This is done by fixing \\(\\theta\\), re-optimizing all other parameters, and checking the resulting log-likelihood.</p>"},{"location":"math/likelihood_profiler/#bracketing-and-optimization","title":"Bracketing and Optimization","text":"<p>The main method <code>profile__()</code> performs a root-finding search using Brent's method to identify interval bounds:</p> <ul> <li>Starts from the MLE and explores a range defined by a multiple of the parameter\u2019s standard deviation.</li> <li>If no solution is found in the initial bracket, the search interval is adaptively expanded up to a configurable number of times\u3010115\u2020source\u3011.</li> <li>Re-optimization is performed at each step using the <code>Optimizer</code> class.</li> </ul> <p>Fallbacks are triggered when optimization fails or bounds do not diverge from the center. In these cases, a Laplace-based interval is returned\u3010115\u2020source\u3011.</p>"},{"location":"math/likelihood_profiler/#grid-profiling","title":"Grid Profiling","text":"<p>The <code>profile()</code> method evaluates the likelihood on a fixed grid around the MLE:</p> <ol> <li>Construct a linearly spaced grid.</li> <li>For each grid point, re-optimizes the model.</li> <li>Interpolates the log-likelihood profile to find the confidence bounds\u3010115\u2020source\u3011.</li> </ol> <p>This variant is slower but does not depend on root-finding convergence.</p>"},{"location":"math/likelihood_profiler/#confidence-interval-estimation","title":"Confidence Interval Estimation","text":"<ul> <li><code>estimate_confint()</code> returns a pair \\((\\theta_{\\text{lower}}, \\theta_{\\text{upper}})\\) for a given confidence level.</li> <li>The default uses 95% confidence, corresponding to a \\(\\chi^2_1\\) threshold of approximately 3.84.</li> <li>Internally computes:</li> </ul> \\[ \\Delta \\log \\mathcal{L} = \\frac{1}{2} \\chi^2_{1, 1 - \\alpha} \\]"},{"location":"math/likelihood_profiler/#parallel-and-batch-profiling","title":"Parallel and Batch Profiling","text":"<ul> <li><code>estimate_confint_all()</code>: loops over parameters sequentially.</li> <li><code>estimate_confint_all_parallel()</code>: uses <code>ThreadPoolExecutor</code> to parallelize across CPU threads\u3010115\u2020source\u3011.</li> <li><code>estimate_confint_all_laplace()</code>: computes intervals using only the Laplace approximation for all parameters, with optional dense or Hutchinson inversion.</li> </ul>"},{"location":"math/likelihood_profiler/#robustness-and-fallbacks","title":"Robustness and Fallbacks","text":"<p>If any profiling fails due to poor optimization convergence or lack of bracketing, a symmetric confidence interval is computed:</p> \\[ \\hat{\\theta} \\pm z_{\\alpha/2} \\cdot \\sqrt{\\text{Var}(\\hat{\\theta})} \\] <p>using the Laplace variance and normal quantiles\u3010115\u2020source\u3011.</p>"},{"location":"math/likelihood_profiler/#references","title":"References","text":"<ul> <li>Pawitan, Y. (2001). In All Likelihood: Statistical Modelling and Inference Using Likelihood. Oxford.</li> <li>Bolker, B. (2008). Ecological Models and Data in R. Princeton University Press.</li> <li>Rasmussen &amp; Williams (2006). Gaussian Processes for Machine Learning. MIT Press.</li> </ul>"},{"location":"math/likelihood_profiler/#source","title":"Source","text":"<p>Defined in <code>likelihood_profiler.py</code>\u3010115\u2020source\u3011.</p>"},{"location":"math/molecular_clock/","title":"Molecular Clock Models","text":""},{"location":"math/molecular_clock/#overview","title":"Overview","text":"<p>Molecular clock models describe how genetic divergence accumulates over time along the branches of a phylogenetic tree. They are essential for calibrating node times and estimating evolutionary rates. The PRISTINE framework implements two main clock models:</p> <ul> <li><code>ConditionalErrorClock</code>: a binomial noise model based on JC69 substitution probabilities.</li> <li><code>ContinuousAdditiveRelaxedClock</code>: a gamma-distributed model for rate heterogeneity.</li> </ul>"},{"location":"math/molecular_clock/#1-conditional-error-clock","title":"1. Conditional Error Clock","text":"<p>This model assumes that the observed distance \\(d\\) between two sequences is a noisy estimate of the true expected distance, which follows a Jukes-Cantor 1969 (JC69) model:</p> \\[ p(l) = \\frac{K - 1}{K} \\left(1 - \\exp\\left(-\\frac{K}{K-1} l\\right)\\right) \\] <p>The likelihood is modeled as a binomial draw:</p> \\[ \\log \\Pr(D \\mid l) = \\log \\binom{L}{L p(d)} + L p(d) \\log p(l) + L (1 - p(d)) \\log(1 - p(l)) \\] <p>where: - \\(L\\) is the number of sites - \\(p(d)\\) is the empirical JC69 probability for observed distance \\(d\\) - \\(p(l)\\) is the expected JC69 probability at branch length \\(l = r \\cdot t\\) (rate \u00d7 duration)</p> <p>This model is used when the observed distance is fixed, and the goal is to estimate branch durations\u3010124\u2020source\u3011.</p>"},{"location":"math/molecular_clock/#2-continuous-additive-relaxed-clock-carc","title":"2. Continuous Additive Relaxed Clock (cARC)","text":"<p>This model assumes that branch-specific substitution rates are gamma-distributed:</p> <ul> <li>Mean rate: \\(\\mu = \\exp(\\text{log\\_rate})\\)</li> <li>Dispersion: \\(\\phi\\)</li> <li>Shape parameter: \\(\\alpha = \\mu \\cdot L \\cdot t \\cdot r\\)</li> <li>Rate parameter: \\(\\beta = \\frac{1}{1 + \\phi}\\)</li> </ul> <p>Then the likelihood of observing a distance \\(d\\) is:</p> \\[ \\log p(d \\mid t) = \\log \\text{Gamma}(d \\cdot L; \\alpha, \\beta) \\] <p>To ensure numerical stability, a barrier penalty is applied to the gamma shape:</p> \\[ \\text{barrier}(x) = \\text{softplus}(-x, \\text{strength}) \\cdot \\text{strength} \\] <p>This model generalizes the strict clock and allows rates to vary across lineages\u3010124\u2020source\u3011.</p>"},{"location":"math/molecular_clock/#simulation","title":"Simulation","text":"<p>The <code>simulate()</code> method in <code>ContinuousAdditiveRelaxedClock</code> draws distances:</p> \\[ d \\sim \\text{Gamma}(\\alpha, \\beta), \\quad d = \\text{distance} / L \\] <p>This supports stochastic modeling of genetic divergence for inference or synthetic data generation.</p>"},{"location":"math/molecular_clock/#api","title":"API","text":""},{"location":"math/molecular_clock/#conditionalerrorclock","title":"ConditionalErrorClock","text":"<ul> <li><code>log_likelihood(durations, distances)</code>: binomial JC69 likelihood</li> <li><code>rate()</code>: returns \\(\\exp(\\text{log\\_rate})\\)</li> </ul>"},{"location":"math/molecular_clock/#continuousadditiverelaxedclock","title":"ContinuousAdditiveRelaxedClock","text":"<ul> <li><code>log_likelihood(durations, distances)</code>: gamma log-likelihood</li> <li><code>simulate(durations)</code>: draw distances from gamma</li> <li><code>rate()</code>: mean substitution rate</li> </ul>"},{"location":"math/molecular_clock/#initialization-utilities","title":"Initialization Utilities","text":"<ul> <li><code>new_conditional_error_clock(K, L)</code>: creates a <code>ConditionalErrorClock</code></li> <li><code>new_continuous_additive_relaxed_clock(L)</code>: creates a <code>ContinuousAdditiveRelaxedClock</code> with default parameters\u3010124\u2020source\u3011</li> </ul>"},{"location":"math/molecular_clock/#references","title":"References","text":"<ul> <li>Jukes, T. H., &amp; Cantor, C. R. (1969). Evolution of protein molecules.</li> <li>Drummond, A. J., &amp; Suchard, M. A. (2010). Bayesian random local clocks.</li> <li>Yang, Z. (2014). Molecular Evolution: A Statistical Approach. Oxford University Press.</li> </ul>"},{"location":"math/molecular_clock/#source","title":"Source","text":"<p>Defined in <code>molecularclock.py</code>\u3010124\u2020source\u3011.</p>"},{"location":"math/optimization/","title":"Optimization in PRISTINE","text":"<p>This document provides a mathematical and algorithmic breakdown of the optimizer used in the PRISTINE framework. The optimizer is designed for maximum-likelihood inference tasks where model parameters are fitted by minimizing a differentiable loss function, typically derived from the negative log-likelihood of observed data under a probabilistic model.</p>"},{"location":"math/optimization/#overview","title":"Overview","text":"<p>The <code>Optimizer</code> class in <code>optimize.py</code> implements robust gradient-based parameter optimization using the Adam algorithm with adaptive learning rate control and backtracking. It targets the minimization of a differentiable loss function:</p> \\[ \\mathcal{L}(\\theta) = - \\log p(\\text{data} \\mid \\theta) \\] <p>where \\(\\theta \\in \\mathbb{R}^n\\) is the vector of free model parameters. The objective is:</p> \\[ \\theta^* = \\arg\\min_\\theta \\mathcal{L}(\\theta) \\] <p>The optimizer operates in discrete steps and includes: - Adam update rule with momentum and adaptive variance normalization - Learning rate backtracking when \\(\\mathcal{L}(\\theta_{t+1}) &gt; \\mathcal{L}(\\theta_t) + \\varepsilon\\) - Early stopping on convergence or too-small learning rate</p>"},{"location":"math/optimization/#mathematical-formulation-and-inner-workings","title":"Mathematical Formulation and Inner Workings","text":""},{"location":"math/optimization/#1-objective","title":"1. Objective","text":"<p>Given a differentiable model loss \\(\\mathcal{L}(\\theta)\\), compute gradients:</p> \\[ g_t = \\nabla_\\theta \\mathcal{L}(\\theta_t) \\]"},{"location":"math/optimization/#2-adam-update-rule","title":"2. Adam Update Rule","text":"<p>Maintain moving averages of gradient and squared gradient:</p> \\[ \\begin{aligned} m_t &amp;= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\ v_t &amp;= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\end{aligned} \\] <p>Bias-corrected estimates:</p> \\[ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} \\] <p>Parameter update:</p> \\[ \\theta_{t+1} = \\theta_t - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} \\] <p>where \\(\\alpha\\) is the learning rate.</p>"},{"location":"math/optimization/#3-loss-evaluation-and-backtracking","title":"3. Loss Evaluation and Backtracking","text":"<p>After updating parameters:</p> <ul> <li>Evaluate \\(\\mathcal{L}(\\theta_{t+1})\\)</li> <li>If \\(\\mathcal{L}(\\theta_{t+1}) &gt; \\mathcal{L}(\\theta_t) + \\varepsilon\\), reduce learning rate \\(\\alpha \\leftarrow \\rho \\alpha\\), revert update, retry</li> </ul> <p>Repeat until: - Improvement is sufficient - Learning rate \\(&lt; \\alpha_{\\min}\\) \u2192 stop</p>"},{"location":"math/optimization/#practical-safeguards","title":"Practical Safeguards","text":"<ul> <li>Gradient sanity checks: abort if NaNs or infinities are found in parameters or gradients</li> <li>Adaptive LR recovery: re-accelerates learning rate when updates stabilize</li> <li>Stopping conditions:<ul> <li>Max iterations</li> <li>Loss convergence</li> <li>Learning rate decay limit</li> </ul> </li> </ul>"},{"location":"math/optimization/#python-implementation-notes","title":"Python Implementation Notes","text":"<ul> <li>Uses <code>torch.optim.Adam</code></li> <li>Backtracking loop wraps optimizer step with retry logic</li> <li>Convergence threshold \\(\\varepsilon\\) and backtrack decay \\(\\rho\\) are configurable</li> </ul>"},{"location":"math/optimization/#applicability","title":"Applicability","text":"<p>This optimizer is ideal for: - Fitting parameters in models with complex loss surfaces - Phylogenetic likelihood inference - Numerical stability in curved or sloppy landscapes</p>"},{"location":"math/pristine_overview/","title":"PRISTINE Framework Overview","text":"<p>PRISTINE is a flexible and differentiable inference engine for phylogenetic and evolutionary modeling. It combines maximum likelihood estimation with robust uncertainty quantification and supports a wide range of biological and statistical analyses.</p> <p>This overview outlines the types of questions and tasks you can perform with PRISTINE, along with links to the technical documentation for each component.</p>"},{"location":"math/pristine_overview/#key-questions-you-can-address","title":"Key Questions You Can Address","text":"<ul> <li>How have sequences evolved over a phylogenetic tree?</li> <li> <p>Use the Felsenstein Pruning Algorithm to compute the likelihood of observed sequences given a substitution model and tree.</p> </li> <li> <p>What substitution process best fits my data?</p> </li> <li> <p>Use the GTR model to estimate transition rates between nucleotides or amino acids.</p> </li> <li> <p>When did evolutionary divergences occur?</p> </li> <li> <p>Fit branch times using molecular clock models, including both strict and relaxed clocks.</p> </li> <li> <p>How do traits or latent states influence speciation and sampling rates?</p> </li> <li> <p>Fit state-dependent diversification processes using birth-death-sampling models, including trait-based linear models.</p> </li> <li> <p>How confident am I in the inferred parameters?</p> </li> <li> <p>Estimate parameter uncertainty with Laplace approximation or robust likelihood profiling.</p> </li> <li> <p>Can I visualize non-identifiability or parameter sloppiness?</p> </li> <li> <p>Use curvature diagnostics in Laplace estimation to assess identifiability.</p> </li> <li> <p>How do I fit models robustly in complex loss landscapes?</p> </li> <li>Use gradient-based optimization with backtracking to ensure convergence even when gradients are unstable.</li> </ul>"},{"location":"math/pristine_overview/#modules-and-their-roles","title":"Modules and Their Roles","text":"Component Functionality Felsenstein Algorithm Efficient likelihood computation over a phylogenetic tree GTR Model Generalized time-reversible substitution process Molecular Clock Models Models that link substitutions to branch time Birth-Death-Sampling Models Diversification modeling over time and states Laplace Estimation Gaussian approximation of posterior uncertainty Likelihood Profiling Profile-based confidence intervals Optimization Stable training via adaptive learning and backtracking"},{"location":"math/pristine_overview/#inference-modes-supported","title":"Inference Modes Supported","text":"<ul> <li>Maximum likelihood estimation</li> <li>Posterior variance approximation</li> <li>Trait-dependent diversification</li> <li>Empirical vs simulated likelihood comparison</li> <li>Parallel profiling and batched optimization</li> </ul>"},{"location":"math/pristine_overview/#ideal-use-cases","title":"Ideal Use Cases","text":"<ul> <li>Phylogenetic inference from DNA, RNA, or protein sequences</li> <li>Molecular dating of trees with uncertain divergence times</li> <li>Parameter sensitivity analysis and identifiability diagnostics</li> <li>Fitting state-dependent speciation models</li> <li>Simulation of evolutionary processes for benchmarking</li> </ul> <p>For in-depth technical details, see the linked module documentation.</p>"},{"location":"tutorials/inference/","title":"PRISTINE Tutorial: End-to-End Phylogenetic Inference","text":"<p>This tutorial introduces the PRISTINE framework through worked examples that guide you through simulation, model construction, and parameter inference for a variety of phylogenetic settings.</p>"},{"location":"tutorials/inference/#sequence-evolution-and-substitution-model-inference","title":"\ud83d\udd01 Sequence Evolution and Substitution Model Inference","text":"<p>Learn how to simulate sequences along a phylogenetic tree and recover GTR substitution parameters:</p> <p>\ud83d\udcc4 example_03_fpa_gtr.py</p> <p>Key steps: - Simulate DNA sequences along a tree using a known GTR model - Build a likelihood function using the Felsenstein pruning algorithm - Optimize the GTR parameters to recover stationary frequencies and exchange rates</p>"},{"location":"tutorials/inference/#molecular-clock-estimation","title":"\ud83d\udd52 Molecular Clock Estimation","text":"<p>Estimate divergence times and substitution rates under different clock models:</p>"},{"location":"tutorials/inference/#continuous-additive-relaxed-clock-carc","title":"Continuous Additive Relaxed Clock (cARC)","text":"<p>\ud83d\udcc4 example_01_carc.py</p> <ul> <li>Simulate distances using a relaxed molecular clock</li> <li>Fit node dates and evolutionary rate using maximum likelihood</li> <li>Compare estimated vs true node ages</li> </ul>"},{"location":"tutorials/inference/#conditional-error-clock-jc69-based","title":"Conditional Error Clock (JC69-based)","text":"<p>\ud83d\udcc4 example_02_cdclock.py</p> <ul> <li>Use a binomial model based on JC69 substitution probabilities</li> <li>Fit branch durations from simulated distances</li> <li>Suitable for shorter sequences or simpler models</li> </ul>"},{"location":"tutorials/inference/#joint-estimation-of-phylogeny-and-divergence","title":"\ud83c\udf33 Joint Estimation of Phylogeny and Divergence","text":"<p>Recover both substitution dynamics and divergence times:</p> <p>\ud83d\udcc4 example_04_fpa_dating.py</p> <ul> <li>Simulate sequences with a known GTR model</li> <li>Fit GTR parameters and internal node dates simultaneously</li> <li>Illustrates parameter entanglement and numerical optimization</li> </ul>"},{"location":"tutorials/inference/#diversification-inference-birth-death-sampling-bds","title":"\ud83c\udf31 Diversification Inference: Birth-Death-Sampling (BDS)","text":""},{"location":"tutorials/inference/#constant-rate-bds","title":"Constant-Rate BDS","text":"<p>\ud83d\udcc4 example_05_bds_constant.py</p> <ul> <li>Simulate trees under a fixed birth and sampling process</li> <li>Estimate log-likelihood using analytic formulas</li> <li>Fit birth and sampling rates from tree shape</li> </ul>"},{"location":"tutorials/inference/#state-dependent-bds","title":"State-Dependent BDS","text":"<p>\ud83d\udcc4 example_06_bds_multistate.py</p> <ul> <li>Simulate sequences that encode hidden states</li> <li>Assign state-dependent birth rates</li> <li>Use ancestral state probabilities to compute likelihoods</li> </ul>"},{"location":"tutorials/inference/#linear-trait-dependent-bds","title":"Linear Trait-Dependent BDS","text":"<p>\ud83d\udcc4 example_07_bds_linear.py</p> <ul> <li>Simulate sequences under a 2-state GTR model</li> <li>Let birth rate depend linearly on hidden traits</li> <li>Estimate trait effects via maximum likelihood</li> </ul>"},{"location":"tutorials/inference/#optimization-and-inference-tools","title":"\ud83e\uddee Optimization and Inference Tools","text":"<p>All examples rely on the robust Adam optimizer with backtracking:</p> <ul> <li>Gradient-based optimization</li> <li>Learning rate adaptation</li> <li>Safe fallback for numerical instability</li> </ul> <p>The optimizer is defined in <code>optimize.py</code>.</p> <p>To assess uncertainty and parameter identifiability: - Use Laplace approximation for posterior variance - Use Likelihood profiling for non-quadratic confidence intervals</p>"},{"location":"tutorials/inference/#next-steps","title":"Next Steps","text":"<p>Explore the overview page for conceptual organization and links to each module\u2019s documentation.</p> <p>All examples are designed to be runnable and modifiable. To explore further: - Change the number of tips or states - Add noise via clock dispersion - Enable curvature diagnostics to test identifiability</p>"}]}